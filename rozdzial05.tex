% !TeX root = ./Dyplom.tex
% chktex-file 9
% chktex-file 21

\chapter{Ocena skuteczności modeli}
	Analizowane algorytmy są nienadzorowane, co oznacza, że nie ma jednoznacznej metody oceny wyników.
	Jedyną metodą ewaluacji modeli tematycznych dającą pewność jest manualna weryfikacja poprzez np.\ ocenianie w trzystopniowej skali,
		które reprezentacje tematów są spójne, a które nie.
	Jest to jednak kosztowna metoda, dlatego tworzy się algorytmy dające dobre przybliżenie ocen człowieka.
	Ocena spójności tematów według takich algorytmów porównywana jest z ocenami człowieka poprzez współczynnik korelacji Pearsona.

	Alternatywną metodą zaproponowaną w~\cite{Word_intrusion} jest podmiana słowa w reprezentacji tematu (ang.\ \emph{word intrusion task}).
	W wygenerowanych zestawach słów podmieniane jest jedno słowo na losowe.
	Następnie uczestnicy badania muszą zidentyfikować podmienione słowo.
	Jeśli tematy są spójne, to niepasujące słowo powinno być łatwe do zidentyfikowania,
		natomiast w mniej spójnych tematach słowa będą sprawiały wrażenie bardziej losowych, co utrudni poprawne rozwiązanie problemu.

\section{Ewaluacja spójności reprezentacji tematów}\label{sec:coherence_scores}
	Najpopularniejszą metodą ewaluacji wygenerowanych tematów jest analiza reprezentacji tematów (zbioru słów opisujących temat).
	Im lepiej słowa opisujące temat reprezentują wszystkie dokumenty w danym temacie, tym lepszy model.
	W pracy~\cite{U_MASS} autorzy proponują algorytm (zwany dalej \(C_{UMass}\))
		analizujący pary słów z reprezentacji tematu pod kątem współwystępowania w dokumentach korpusu.
	Jeśli para słów często występuje razem w dokumentach, to są one powiązane.
	Można więc określić spójność całej reprezentacji tematu badając wszystkie w niej zawarte pary słów.
	Miara ta porównana została z wynikami zadania podmiany słowa i uzyskała ona lepszą korelację z ocenami ludzi.

	Autorzy~\cite{C_NPMI} proponują rozwiązanie bazujące na wektorach kontekstowych słów tematu (zwane dalej \(C_{NPMI}\)).
	Wektory te są obliczane na podstawie współwystępujących słów w kontekście analizowanego słowa.
	Kontekst jest definiowany jako tzw.\ przesuwne okno (ang.\ \emph{sliding window}) zwierające \(\pm 5\) otaczających słów.
	Następnie wektory są ważone algorytmem NPMI (ang.\ \emph{Normalized Point Mutual Information}).
	Końcowa wartość obliczana jest jako średnia odległość kosinusowa między wszystkimi parami wektorów kontekstowych słów tematu.

	W pracy~\cite{Eval_Topics} autorzy stworzyli bazową strukturę pozwalającą na zdefiniowanie metryk \(C_{UMass}\), \(C_{NPMI}\)
		oraz innych miar spójności w jednej przestrzeni.
	W projekcie skorzystano z biblioteki \verb|Gensim|\footnote{\url{https://radimrehurek.com/gensim/models/coherencemodel.html} (dostęp dnia 29 maja 2021)},
		która implementuje miary spójności na podstawie tych definicji.

\section{Ewaluacja spójności rozkładu tematów}\label{sec:consistency_scores}
	Ze względu na specyfikę korpusu możliwe jest również sformułowanie metryki wykorzystującej porządek obrad sejmu.
	Każde posiedzenie przebiega zgodnie z ustaloną strukturą, gdzie marszałek zarządza obrady na temat danej ustawy, poprawki czy interpelacji,
		a następnie kolejni posłowie wyrażają swoją opinię na ten temat.
	Można więc założyć, że w obrębie debaty nad jednym punktem obrad, wypowiedzi powinny zostać zakwalifikowane do tego samego zbioru tematycznego.
	Stworzono więc algorytm wykorzystujący tę zależność, przedstawiony na listingu~\ref{lst:consistency_alg}.
	Implementacja algorytmu w języku Python pokazana jest na listingu~\ref{lst:score_consistency}.
	Założono, że w zdecydowanej większości przypadków, kolejne dwie wypowiedzi powinny mieć przypisany ten sam temat.

	Algorytm analizuje osobno każdy dzień posiedzeń.
	Wypowiedzi sortowane są zgodnie z kolejnością wynikającą z transkrypcji przemówień.
	Jeśli w ciągu dnia jest mniej niż dwie wypowiedzi, to dzień jest pomijany.
	W zbiorze danych występuje jeden taki przypadek, natomiast mediana wynosi 138.
	Wśród wypowiedzi danego dnia porównywane są tematy par sąsiadujących wypowiedzi.
	Zliczane są zarówno znalezione pasujące pary, jak i różniące się.
	Jeśli temat analizowanej wypowiedzi jest nieznany (algorytm HDBSCAN nie przypisał dokumentu do żadnej grupy),
		to jest ona pomijana.
	Mechanizm ten zapobiega sytuacji, w której duża liczba nieprzypisanych dokumentów zawyżałaby wynik.
	Dzieje się tak ze względu na to, że nieprzypisane dokumenty często stanowią znaczną część zbioru (30--50\%),
		przez co istnieje duże prawdopodobieństwo znalezienia par takich dokumentów.
	Dodatkowo, od sumy różniących się par odejmowana jest liczba o 1 mniejsza od liczby tematów danego dnia.
	Działanie to rekompensuje przejścia między tematami, które muszą wystąpić i nie są błędem.
	Takich przejść w ciągu dnia wystąpi co najmniej tyle, ile jest tematów minus jeden.
	Końcowy wynik obliczany jest jako stosunek zgadzających się par do wszystkich zliczonych par.
	
	\begin{lstlisting}[style=algorithm,label=lst:consistency_alg,caption=Algorytm obliczania spójności tematów]
input: speeches sorted by original order of speaking, with date and assigned topic identifier
output: consistency score between 0 and 1 with higher score indicating more consistent topics
	
same_per_day := []
different_per_day := []

for each speeches in data grouped by date do
	if len(speeches) < 2
		continue
	
	same := 0
	different := 0
	for i in range(len(speeches) - 1) do
		if speeches[i].topic is unknown
			continue

		if speeches[i].topic = speeches[i + 1].topic
			same := same + 1
		else
			different := different + 1

	topic_count := len(speeches.topic.unique())
	different := different - (topic_count - 1)
	
	same_per_day.append(same)
	different_per_day.append(different)

score = sum(same_per_day) / sum(same_per_day, different_per_day)
return score
	\end{lstlisting}

\section{Końcowa ocena}
	Powyższe metody nie poddają ocenie samej liczby tematów i liczby nieprzyporządkowanych dokumentów.
	Z tego powodu, wysoki wynik mógłby osiągnąć model, który znalazłby zaledwie 5 tematów.\todo{Pokazać jakieś dane-rysunek albo tabelka}
	Taki model przyporządkuje wszystkie dokumenty generując bardzo ogólne tematy.
	Podobnie wysoki wynik osiągnie model, który wygeneruje tysiące bardzo szczegółowych tematów.
	Takie modele można odfiltrować definiując odpowiednie progi, jednak taka metoda wymusza ustanowienie założeń,
		które niekoniecznie będą optymalne.
	Zamiast tego, stworzone zostały dwie dodatkowe miary: jedna ocenia liczbę przyporządkowanych dokumentów \(s_f\), druga ocenia liczbę tematów \(s_T\).
	Końcowa ocena modelu jest średnią ocen \(C_{UMass}\), \(C_{NPMI}\), \(s_c\), \(s_f\) oraz \(s_T\) przeskalowanych do pełnego zakresu (0, 1).
	Skalowanie odbywa się przy pomocy klasy \verb|MinMaxScaler| z biblioteki \verb|scikit-learn|,
		jak przedstawiono w linijce 18 listingu~\ref{lst:final_score}.

	\subsection{Ocena liczby przyporządkowanych dokumentów}\label{sec:score_not_found}
		Miara oceniająca liczbę przyporządkowanych dokumentów korzysta bezpośrednio z wyniku grupowania.
		Grupa oznaczona jako '-1' oznacza dokumenty, które nie zostały zgrupowane, a więc nie mają przypisanego tematu.
		Rozmiar tej grupy odejmowany jest od rozmiaru korpusu uzyskując liczbę dokumentów z rozpoznanymi tematami.
		Liczba ta dzielona jest przez rozmiar korpusu, co daje miarę w przedziale (0, 1).
		Działanie to przedstawione jest w linijce 16 listingu~\ref{lst:final_score},
			gdzie \verb|scores['not_found']| to kolumna tabeli zawierająca rozmiar grupy nieprzyporządkowanych dokumentów dla każdego modelu.
		
		Przykładowy wynik działania tej metody przedstawiony został na rysunku~\ref{fig:not_found}.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=\linewidth]{rys05/not_found.png}
			\caption{Liczba rozpoznanych dokumentów przed i po znormalizowaniu dla wybranych parametrów; format osi x: <n\_neighbors>: <min\_cluster\_size>, <min\_samples>}\label{fig:not_found}
		\end{figure}

	\subsection{Ocena liczby tematów}
		Dużo trudniej jest ocenić liczbę tematów, gdyż zarówno osiemdziesiąt ogólnych tematów,
			jak i tysiąc szczegółowych mogą zostać uznane za odpowiednie.
		Zważając jednak na specyfikę korpusu, przyjęto że różnorodność poruszanych kwestii powinna skutkować relatywnie dużą liczbą tematów.
		Z tego powodu uznano, że zbyt ogólne tematy nie są odpowiednie w tym kontekście.
		Nadmiernie szczegółowe, a więc o małej liczebności tematy również tracą informacje o zależnościach między dokumentami,
			więc także uznane zostały za niepożądane.
		W takim wypadku za odpowiednią liczbę tematów przyjęto medianę liczby tematów dla wszystkich testowanych modeli, która wyniosła 219.

		W celu określenia wyniku każdego modelu wykorzystano zmodyfikowaną standaryzację Z.
		Standaryzacja Z obliczana jest zgodnie z następującym wzorem:
		\[z=\frac{x-\mu}{\sigma}\]
		gdzie:
		\begin{itemize}
			\item \(x\) wartość zmiennej,
			\item \(\mu\) średnia z populacji,
			\item \(\sigma\) odchylenie standardowe populacji.
		\end{itemize}
		
		Średnia arytmetyczna i odchylenie standardowe są jednak podatne na wpływ wartości leżących daleko od średniej.
		Z tego powodu często wykorzystywana jest zmodyfikowana standaryzacja Z, która mierzy odległości od mediany:
		\[\tilde{z}=\frac{x-\tilde{x}}{k\cdot MAD}\]
		gdzie:
		\begin{itemize}
			\item \(x\) wartość zmiennej,
			\item \(\tilde{x}\) mediana z populacji,
			\item \(k=1.486\) wartość korekcyjna stanowiąca przybliżenie odchylenia standardowego,
			\item \(MAD\) średnie odchylenie bezwzględne --- \(median\left(|x_i-\tilde{x}|\right)\).
		\end{itemize}

		Tak ustandaryzowane dane rozłożone są wokół zera, gdzie liczby tematów mniejsze od mediany mają ujemny wynik, a większe dodatni.
		Takie odległości interpretowane są jako odległości od najlepszej liczby tematów,
			im większa odległość tym gorszy powinien być wynik takiego modelu.
		W celu dodatkowego ograniczenia wpływu pomiarów zwracających błędną liczbę tematów, medianę obliczono dla ograniczonego zakresu (50, 1000).
		Aby wyrazić te odległości w skali (0, 1), gdzie 1 to najlepszy wynik, znormalizowano dane
			poprzez odwrócenie wartości bezwzględnej standaryzacji Z powiększonej o 1 (aby minimalna wartość była równa 1):
		\[s_T=\frac{1}{|\tilde{z}|+1}\]
		Implementacja tej miary przedstawiona została na listingu~\ref{lst:final_score} jako funkcja \verb|modified_zscore|.
		Wykres liczby tematów dla wybranego fragmentu parametrów przed i po znormalizowaniu przedstawiony został na rysunku~\ref{fig:topic_counts}.

		\begin{figure}[htb]
			\centering
			\includegraphics[width=\linewidth]{rys05/topic_counts.png}
			\caption{Liczba tematów przed i po znormalizowaniu dla wybranych parametrów; format osi x: <n\_neighbors>: <min\_cluster\_size>, <min\_samples>}\label{fig:topic_counts}
		\end{figure}

	\subsection{Implementacja}
		Algorytmy zaimplementowane zostały w języku Python.
		Wykorzystane zostały biblioteki \verb|numpy|, \verb|pandas|, \verb|scikit-learn| oraz \verb|scipy|.

		\begin{lstlisting}[label=lst:score_consistency,language=Python,caption=Funkcja obliczająca spójność s\_c]
def score_consistency(df: pd.DataFrame) -> float:
	same_per_day = []
	different_per_day = []

	for _, day in df.sort_values('speech_order').groupby(by='date'):
		if len(day) < 2:
			continue

		same = 0
		different = 0
		for i in range(len(day) - 1):
			if day.iloc[i].topic == -1:
				continue

			if day.iloc[i].topic == day.iloc[i + 1].topic:
				same += 1
			else:
				different += 1

		topics = len(day.topic.unique())
		different -= (topics - 1)

		same_per_day.append(same)
		different_per_day.append(different)

	s_c = np.sum(same_per_day) / np.sum([*same_per_day, *different_per_day])

	return s_c
		\end{lstlisting}

		\begin{lstlisting}[label=lst:final_score,language=Python,caption=Obliczanie finalnej oceny spójności tematów dla listy modeli]
from sklearn.preprocessing import MinMaxScaler
from scipy import stats
import numpy as np

def modified_zscore(x: List[float]) -> List[float]:
	valid_x = [count for count in x if count > 50 and count < 1000]
	median = np.median(valid_x)
	deviation_from_med = x - median

	mad = np.median(np.abs(deviation_from_med))

	consistency_correction = 1.4826
	mod_zscore = deviation_from_med / (consistency_correction * mad)

	return mod_zscore

scores['s_f'] = [(total - x) / total for x in scores['not_found']]

scores['s_T'] = 1 / (np.abs(modified_zscore(scores['topic_count'])) + 1)

scaler = MinMaxScaler()
s = scaler.fit_transform(scores[['u_mass','c_npmi','s_c','s_f','s_T']])

scores['avg'] = np.nanmean(s, axis=1)
		\end{lstlisting}

\section{Przeprowadzenie pomiarów}
	Pomiary metodami opisanymi w poprzednim rozdziale zaplanowano tak, by wyniki umożliwiły jak najszerszą analizę porównawczą.
	Wektory zdań generowano następującymi metodami opisanymi w rozdziale~\ref{sec:vectors}:
	\begin{itemize}
		\item Sentence-Transformers,
		\item Universal Sentence Encoder,
		\item TF-IDF\@.
	\end{itemize}
	Dodatkowo, dla pierwszych dwóch metod wykonano pomiary dla następujących sposobów łączenia wektorów zdań w wektory wypowiedzi (rozdział~\ref{sec:sentences}):
	\begin{itemize}
		\item LexRank (top1, top5, średnia ważona),
		\item TF-IDF (top1, top5, średnia ważona),
		\item średnia arytmetyczna,
	\end{itemize}
	oraz bez podziału na zdania (generowanie wektora bezpośrednio dla całej wypowiedzi).
	Dla wektorów tf-idf przygotowano dwa warianty: jeden analizujący nieprzetworzone dokumenty
		oraz drugi wykorzystujący dane przetworzone w sposób opisany w rozdziale~\ref{sec:tfidf_vectors}.

	Redukcję wektorów wykonywano do pięciu wymiarów metryką kosinusową dla wektorów \emph{SBERT} i \emph{USE} oraz metryką Hellingera dla wektorów \emph{TF-IDF}.
	Zmieniano jedynie parametr \verb|n_neighbors| testując następujące wartości: 5, 10, 15, 20, 25 oraz 50, gdzie najmniejsze wartości skupiają się na lokalnych zależnościach,
		a większe wykrywają globalne struktury.
	Podczas klastrowania algorytmem HDBSCAN testowane były parametry \verb|min_cluster_size| oraz \verb|min_samples| w kombinacjach przedstawionych w tabeli~\ref{tab:hdbscan_params}.

	\begin{table}[htb]
		\caption{Testowane parametry algorytmu HDBSCAN}\label{tab:hdbscan_params}
		\centering
		\begin{tabular}{lrrrrrrrrrrrrrr}
			\toprule
			min\_cluster\_size	& 10 & \multicolumn{2}{l}{20} &  \multicolumn{3}{l}{50} & \multicolumn{4}{l}{100}	&  \multicolumn{4}{l}{200} \\
			min\_samples 				& 10 &  10 &  20 							&  10 &  25 &  50 				& 	1 &	10 &	50 &  100		&   1 &   20 &  100 &  200 \\
			\bottomrule
		\end{tabular}
	\end{table}

	Ze względu na złożoność obliczeniową pomiarów, w pierwszej kolejności przeprowadzono pomiary na ograniczonym zakresie:
	\begin{itemize}
		\item \verb|n_neighbors|: 5, 10, 15, 20
		\item \verb|min_cluster_size|: 10, 20, 50, 100
	\end{itemize}
	oraz pominięto liczenie wyniku \(C_{NPMI}\).
	W tabeli~\ref{tab:avg_scores}a przedstawiono posortowane, uśrednione wyniki \(C_{UMass}\), \(s_c\), \(s_f\) oraz \(s_T\).
	Na podstawie tych wyników zdecydowano się odrzucić metody bazujące na najlepszych wektorach wg.\ rankingu (top1 i top5),
		ponieważ zarówno dla algorytmu LexRank, jak i TF-IDF dawały one zauważalnie gorsze wyniki od pozostałych metod.
	Z tego powodu nie zostały one uwzględnione podczas obliczania wyników dla pełnego zakresu parametrów.
	Uśrednione wyniki na pełnym zakresie, uwzględniające również metodę \(C_{NPMI}\), przedstawione zostały w tabeli~\ref{tab:avg_scores}b.
	Dalsze omówienie tych wyników zawarte jest w kolejnym rozdziale.
		
	\begin{table}[htb]
		\caption{Średnie wyniki dla zakresu parametrów: a) ograniczonego, b) pełnego}\label{tab:avg_scores} % chktex 9 chktex 10
		\centering
		\begin{minipage}[t]{.5\textwidth}
			a)\par\medskip % chktex 10
			\begin{tabular}{lr}
				\toprule
								metoda &	średni wynik \\
				\midrule
								sbert-default & 0.412 \\
						sbert-tf-idf-top1 & 0.416 \\
							use-lexrank-top5 & 0.459 \\
						sbert-lexrank-top1 & 0.463 \\
					use-lexrank-weighted & 0.465 \\
							use-lexrank-top1 & 0.489 \\
							use-tf-idf-top5 & 0.501 \\
						sbert-tf-idf-top5 & 0.505 \\
												tfidf & 0.516 \\
					use-tf-idf-weighted & 0.522 \\
						sbert-lexrank-top5 & 0.523 \\
									tfidf-spacy & 0.558 \\
											use-mean & 0.559 \\
				sbert-tf-idf-weighted & 0.565 \\
				sbert-lexrank-weighted & 0.572 \\
										sbert-mean & 0.587 \\
									use-default & 0.593 \\
				\bottomrule
			\end{tabular}
		\end{minipage}%
		\begin{minipage}[t]{.5\textwidth}
			b)\par\medskip % chktex 10
			\begin{tabular}{lr}
				\toprule
								metoda &	średni wynik \\
				\midrule
								sbert-default & 0.331 \\
					use-lexrank-weighted & 0.426 \\
					use-tf-idf-weighted & 0.525 \\
				sbert-tf-idf-weighted & 0.531 \\
				sbert-lexrank-weighted & 0.545 \\
											use-mean & 0.551 \\
										sbert-mean & 0.557 \\
									use-default & 0.558 \\
												tfidf & 0.580 \\
									tfidf-spacy & 0.617 \\
				\bottomrule
				\end{tabular}
		\end{minipage}
	\end{table}

	\subsection{Optymalizacje}
		Ze względu na złożoność pamięciową algorytmów badających spójność reprezentacji tematów (rozdział~\ref{sec:coherence_scores}),
			algorytmy te badały ograniczony zbiór dokumentów.
		Aby jednak zachować zależności między dokumentami na pełnym zbiorze, w pierwszym kroku algorytm HDBSCAN grupuje wszystkie dokumenty.
		Dopiero z pogrupowanych dokumentów losowane są próbki według zdefiniowanych \(test\_sample\) oraz \(test\_sample\_frac\),
			które odpowiednio oznaczają minimalną liczbę próbek z każdej grupy, oraz część grupy.
		Próbki losowane są w zależności od rozmiaru grupy \(N\):
		\begin{itemize}
			\item jeśli \(N\) jest większe niż \(\frac{test\_sample}{test\_sample\_frac}\), to losujemy \(test\_sample\_frac \cdot N\) próbek,
			\item jeśli \(N\) jest większe niż \(test\_sample\), to losujemy \(test\_sample\) próbek,
			\item jeśli \(N\) jest mniejsze niż \(test\_sample\), to bierzemy wszystkie \(N\) próbek.
		\end{itemize}
		Ustawiając \(test\_sample=100\) oraz \(test\_sample\_frac=0.2\) sprawiono,
			że brane pod uwagę jest 20\% dokumentów z każdej grupy, ale nie mniej niż 100 (lub całość grupy).
		Dolne ograniczenie \(test\_sample\) sprawia, że nie zawsze analizowane jest tyle samo dokumentów.
		W zależności od liczby wykrytych tematów, analizowanych jest średnio od 60 do 90 tysięcy próbek (tabela~\ref{tab:support}).
		Przeprowadzenie klastrowania na pełnym zbiorze danych umożliwia również poprawne przeprowadzenie testów spójności rozkładów tematów (rozdział~\ref{sec:consistency_scores})
			na wszystkich dokumentach, co jest wymogiem algorytmu (ponieważ potrzebujemy wszystkie pary występujące zaraz po sobie).
		
		\begin{table}[htb]
			\caption{Liczba analizowanych próbek w zależności od liczby wykrytych tematów (mediana)}\label{tab:support}
			\centering
			\begin{tabular}{lrl}
				\toprule
				liczba tematów  &  liczba próbek	&	\% korpusu \\
				\midrule
				(4 74]     &  58652 &	20.1\% \\
				(74 107]   &  59586 &	20.4\% \\
				(107 136]  &  61376 &	21.1\% \\
				(136 175]  &  63410 &	21.8\% \\
				(175 216]  &  65728 &	22.6\% \\
				(216 260]  &  67814 &	23.3\% \\
				(260 363]  &  70716 &	24.3\% \\
				(363 502]  &  75451 &	25.9\% \\
				(502 791]  &  82544 &	28.3\% \\
				(791 3166] &  92502 &	31.7\% \\
				\bottomrule
			\end{tabular}
		\end{table}

\section{Ewaluacja LDA}
	Tematy uzyskane metodą BERTopic zostały porównane z popularną metodą analizy tematycznej --- LDA\@.
	W tym celu wykonano pomiary spójności algorytmami opisanymi w poprzednich rozdziałach.
	Jedną z zasadniczych wad algorytmu LDA jest fakt, iż liczba tematów jest jego parametrem.
	Aby znaleźć optymalną liczbę tematów, przeprowadzono pomiary dla różnych wartości parametru z zakresu [10--300].
	Dokumenty przetworzono tą samą metodą, co w przypadku generowania wektorów TF-IDF w rozdziale~\ref{sec:tfidf_vectors},
		mianowicie lematyzacja słownikami \emph{Morfeusz2}, usunięcie polskich słów bez znaczenia (ang.\ \emph{stop words}) oraz wygenerowanie bigramów.
	Ze względu na ograniczenia techniczne związane z pamięcią operacyjną maszyny (128GB), analizie poddano sto tysięcy losowo wybranych dokumentów,
		co stanowi ok.~34\% całości korpusu.
	Wyniki tych pomiarów przedstawione są na rysunku~\ref{fig:lda_scores} oraz w tabeli~\ref{tab:lda_scores}.
	Z danych można odczytać, że dokładność modelu utrzymuje się na podobnym poziomie do ok.~80 tematów, potem zaczyna gwałtownie spadać.
	Za optymalną liczbę tematów wybrano więc 80 ze względu na zróżnicowanie korpusu, który z pewnością zawiera dużo tematów.

	\begin{figure}[htb]
		\centering
		\begin{minipage}{.5\textwidth}
			a)\par\medskip % chktex 10
			\includegraphics[width=\linewidth]{rys05/lda_c_npmi.png}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			b)\par\medskip % chktex 10
			\includegraphics[width=\linewidth]{rys05/lda_u_mass.png}
		\end{minipage}
		\caption{Wyniki pomiarów dla LDA:\ a) \(C_{NPMI}\), b) \(C_{UMass}\)}\label{fig:lda_scores} % chktex 9 chktex 10
	\end{figure}

	\begin{table}[htb]
		\centering
		\caption{Wyniki pomiarów dla LDA w zależności od liczby tematów}\label{tab:lda_scores}
		\begin{tabular}{lrr}
			\toprule
			{} &    c\_npmi &    u\_mass \\
			\midrule
			10  &  0.071499 & -0.696868 \\
			20  &  0.076610 & -0.694938 \\
			30  &  0.071255 & -0.645520 \\
			40  &  0.072496 & -0.730301 \\
			50  &  0.068460 & -0.696550 \\
			60  &  0.073730 & -0.692332 \\
			70  &  0.073619 & -0.799463 \\
			80  &  0.067665 & -0.813064 \\
			90  &  0.049716 & -1.132477 \\
			100 &  0.036761 & -1.366198 \\
			150 &  0.011804 & -2.110305 \\
			200 &  0.014556 & -2.170508 \\
			250 & -0.037109 & -4.191849 \\
			300 & -0.038166 & -4.027447 \\
			\bottomrule
		\end{tabular}
	\end{table}

	Dla znalezionych tematów wygenerowano reprezentacje składające się z pięciu najwyżej ocenianych słów.
	Losowo wybrane 5 tematów przedstawiono w tabeli~\ref{tab:lda_topics}a.
	Dodatkowo, ze względu na to, iż mediana liczby znalezionych tematów algorytmem HDBSCAN wyniosła 219,
		postanowiono zbadać również model LDA dla 200 tematów (tabela~\ref{tab:lda_topics}b).
	Dla obu tych modeli widoczny jest problem opisany na początku rozdziału~\ref{sec:topic_modeling}.
	Mianowicie, reprezentacje tematów zawierają wiele podobnych słów specyficznych dla całego korpusu, a nie tematu (,,ustawa'', ,,minister'', czy ,,poseł'').
	Podjęto próby poszerzenia listy słów nieinformatywnych, jednak szybko okazało się, iż takich słów jest zbyt wiele
		--- po usunięciu słów specyficznych dla korpusu nadal reprezentacje tematów składały się często ze słów typu ,,czas'', ,,kwestia'', ,,chodzi''.
	Nie stwierdzono zauważalnej różnicy między dokładnością reprezentacji wygenerowanych dla 80 tematów, a tymi dla 200.
	
	\begin{table}[htb]
		\caption{Reprezentacje pięciu losowo wybranych tematów dla LDA:\ a) 80 tematów, b) 200 tematów}\label{tab:lda_topics} % chktex 9 chktex 10
		\centering
		a) % chktex 10
		\begin{tabularx}{.7\textwidth}{rl}
			\toprule
			A	&	rok | mieć | europejski | minister | r	\\ 
			B	&	państwo | ustawa | mieć | rok | Polska	\\ 
			C	&	minister | państwo | dziecko | rok | szkoła	\\
			D	& minister | poseł | pytanie | mieć | chcieć	\\
			E	& ustawa | mieć | poseł | wysoki | poprawka	\\
			\midrule
		\end{tabularx}

		b) % chktex 10
		\begin{tabularx}{.7\textwidth}{rl}
			A	&	ustawa | projekt | ustawy | komisja | wysoki	\\ 
			B	&	rok | mieć | minister | poseł | Maryja \\ 
			C	&	ustawa | zmiana | projekt | prawo | ustawy \\
			D	&	trybunał | konstytucyjny | ustawa | rok | Trybunału	\\
			E	&	ustawa | mieć | rok | Pan | sprawa	\\
			\bottomrule
		\end{tabularx}
	\end{table}
